_wandb:
    value:
        cli_version: 0.18.0
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 9
                - 11
                - 41
                - 49
                - 55
                - 63
                - 71
                - 83
                - 103
            "2":
                - 1
                - 9
                - 11
                - 41
                - 49
                - 55
                - 63
                - 71
                - 83
                - 103
            "3":
                - 7
                - 13
                - 23
                - 55
                - 66
            "4": 3.11.13
            "5": 0.18.0
            "6": 4.44.2
            "8":
                - 5
            "12": 0.18.0
            "13": darwin-arm64
absolute_clamp:
    value: 0
batch_size_per_device:
    value: 32
clamp_futures_grad:
    value: false
clamp_futures_grad_max_change:
    value: 9
clamp_max_after_warm_up:
    value: 0
context_length:
    value: 256
contrastive_loss:
    value: false
contrastive_loss_coeff:
    value: 0.0005
dataset_dir:
    value: ./livebench_coding
dataset_name:
    value: livebench_coding
debug_unused_parameters:
    value: false
denoising_initial_condition:
    value: random_noise
dyt_alpha_init:
    value: 0.5
ebt_act_func:
    value: silu
ebt_norm:
    value: rms
ebt_type:
    value: time_embed
embedding_dim:
    value: 384
execution_mode:
    value: pretrain
ffn_dim_multiplier:
    value: 1
gaussian_random_noise_scaling:
    value: 1
langevin_dynamics_noise:
    value: 0
langevin_dynamics_noise_learnable:
    value: false
lr:
    value: 0.001
max_steps:
    value: 100000
mcmc_num_steps:
    value: 2
mcmc_replay_buffer:
    value: false
mcmc_step_size:
    value: 500
mcmc_step_size_learnable:
    value: true
mcmc_step_size_lr_multiplier:
    value: 1500
model_name:
    value: ebt
multiheaded_attention_heads:
    value: 6
no_mcmc_detach:
    value: false
normalize_initial_condition:
    value: true
normalize_initial_condition_only_first_step:
    value: false
num_modality_processing_mlp_layers:
    value: 1
num_transformer_blocks:
    value: 6
num_workers_per_gpu:
    value: 12
pretokenize_dataset:
    value: true
randomize_mcmc_num_steps:
    value: 0
randomize_mcmc_num_steps_final_landscape:
    value: false
randomize_mcmc_num_steps_min:
    value: 0
randomize_mcmc_step_size_scale:
    value: 1
reconstruction_coeff:
    value: 1
sharpen_predicted_distribution:
    value: 0
soften_target_prob_dist:
    value: 0
tokenizer:
    value: EleutherAI/gpt-neox-20b
truncate_mcmc:
    value: false
vocab_to_embed_uses_prob_dist:
    value: false
weight_initialization_gain:
    value: 1
weight_initialization_method:
    value: xavier
